version: "3.9"

services:
  # -------------------------------------------
  # 1) Ollama LLM server (GPU support optional)
  # -------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks: [ollama-net]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: ["serve"]
    volumes:
      - ./ollama/models:/root/.ollama
    ports:
      - "11434:11434"

  # -------------------------------------------------
  # 2) LiteLLM proxy exposing OpenAI-compatible API
  # -------------------------------------------------
  litellm:
    image: ghcr.io/berriai/litellm:main
    container_name: litellm
    depends_on: [ollama]
    networks: [ollama-net]
    command: ["--model", "ollama/llama2", "--api_base", "http://ollama:11434", "--port", "4000"]
    environment:
      - OPENAI_API_TYPE=openai
      - OPENAI_API_KEY=dummy-key
    ports:
      - "4000:4000"

  # -------------------------------------------------
  # 3) Open-WebUI chat interface (optional)
  # -------------------------------------------------
  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    restart: unless-stopped
    depends_on: [ollama]
    networks: [ollama-net]
    volumes:
      - ./ollama/webui:/app/backend/data
    ports:
      - "8080:8080"          # browse http://localhost:8080
    environment:
      - OLLAMA_BASE_URLS=http://ollama:11434
      - WEBUI_AUTH=False
      - WEBUI_NAME=Ollama
      - WEBUI_URL=http://localhost:8080

  # -------------------------------------------------
  # 4) OpenEvolve running the Snake example
  # -------------------------------------------------
  alphaevolve:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: alphaevolve
    depends_on: [litellm]
    networks: [ollama-net]
    environment:
      - OPENAI_API_BASE=http://litellm:4000
      - OPENAI_API_KEY=dummy-key
    volumes:
      - ./alphaevolve/.tmp_runs:/workspace/.tmp_runs
    command:
      - examples/snake/npc_player.py
      - examples/snake/evaluator.py
      - -c
      - examples/snake/snake_config.yaml
    tty: true

networks:
  ollama-net: {}
